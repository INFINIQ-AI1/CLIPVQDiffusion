{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo For text to image generation using CLIP VQDiffusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "from einops import einsum \n",
    "from image_synthesis.utils.io import load_yaml_config\n",
    "from image_synthesis.modeling.build import build_model\n",
    "from image_synthesis.utils.misc import get_model_parameters_info, instantiate_from_config\n",
    "\n",
    "class CLIPVQDiffusion():\n",
    "    def __init__(self, config, path, imagenet_cf=False):\n",
    "        self.info = self.get_model(ema=True, model_path=path, config_path=config)\n",
    "        self.model = self.info['model']\n",
    "        self.tokenizer = self.info['tokenizer']\n",
    "        self.model = self.model.cuda()\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters(): \n",
    "            param.requires_grad=False\n",
    "        self.cos_sim=list()\n",
    "        self.total_cnt=0\n",
    "\n",
    "    def get_model(self, ema, model_path, config_path):\n",
    "        if 'OUTPUT' in model_path: # pretrained model\n",
    "            model_name = model_path.split(os.path.sep)[-3]\n",
    "        else: \n",
    "            model_name = os.path.basename(config_path).replace('.yaml', '')\n",
    "            \n",
    "        config = load_yaml_config(config_path)\n",
    "        model = build_model(config)\n",
    "        tokenizer = instantiate_from_config(config[\"dataloader\"][\"validation_datasets\"][0][\"params\"][\"tokenize_config\"])\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            ema_model = model.get_ema_model()\n",
    "            model_ckpt = torch.load(model_path)\n",
    "            missing, unexpected = ema_model.load_state_dict(model_ckpt)#, strict=False)\n",
    "        else:\n",
    "            print(\"Model path: {} does not exist.\".format(model_path))\n",
    "            exit(0)\n",
    "        \n",
    "        return {'model': model, 'tokenizer':tokenizer}\n",
    "\n",
    "    def inference_generate_sample_with_condition(self, data, truncation_rate, batch_size, guidance_scale=1.0, prior_rule=0, prior_weight=0, learnable_cf=True):\n",
    "        self.model.guidance_scale = guidance_scale\n",
    "        self.model.learnable_cf = self.model.transformer.learnable_cf = learnable_cf # whether to use learnable classifier-free\n",
    "        self.model.transformer.prior_rule = prior_rule      # inference rule: 0 for VQ-Diffusion v1, 1 for only high-quality inference, 2 for purity prior\n",
    "        self.model.transformer.prior_weight = prior_weight  # probability adjust parameter, 'r' in Equation.11 of Improved VQ-Diffusion\n",
    "\n",
    "        # to device \n",
    "        data['text']['token'] = data['text']['token'].squeeze(1).to(self.model.device)\n",
    "        data['text']['mask'] = data['text']['mask'].to(self.model.device)\n",
    "        \n",
    "        add_string = 'r'\n",
    "        with torch.no_grad():\n",
    "            model_out = self.model.generate_content(\n",
    "            batch=data,\n",
    "            filter_ratio=0,\n",
    "            replicate=batch_size,\n",
    "            content_ratio=1,\n",
    "            return_att_weight=False,\n",
    "            sample_type=\"top\"+str(truncation_rate)+add_string,\n",
    "            ) # B x C x H x W\n",
    "\n",
    "        # save results\n",
    "        content = model_out['content']\n",
    "        clip_text_embedding = self.model.condition_codec.encode_text(data[\"text\"][\"token\"])\n",
    "        clip_img_embedding = self.model.condition_codec.encode_image(self.model.clip_img_preprocess(torchvision.transforms.Resize(224)((content/255))))\n",
    "        \n",
    "        clipscore = einsum(clip_text_embedding, clip_img_embedding, \"b d, b d -> b\")\n",
    "        print(f'clip_score:{clipscore}')\n",
    "        return content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    }
   ],
   "source": [
    "# hyper parameter and model setting \n",
    "\n",
    "CONFIG_PATH = \"./configs/coco_vqd_tune_test.yaml\" \n",
    "MODEL_PATH = f\"./OUTPUT/pretrained_model/diffusion_models/clipvq_coco.pt\"\n",
    "TRUNCATION_RATE = 0.85\n",
    "SAVE_ROOT = f\"./result/ffhq_tune_test2/0.5\"\n",
    "TIMES = 16 # repetition time\n",
    "GUIDANCE_SCALE= 1.15\n",
    "PRIOR_RULE=0 \n",
    "PRIOR_WEIGHT=0 \n",
    "LEARNABLE_CF=True\n",
    "\n",
    "config = load_yaml_config(CONFIG_PATH)\n",
    "diff_model = CLIPVQDiffusion(config=CONFIG_PATH, path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "clip_score:tensor([0.3251], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/team_ai/anaconda3/envs/HSD_vqd/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# generate image \n",
    "\n",
    "text = \"a photograph of a apple and banana in the kitchen\"\n",
    "data=dict()\n",
    "data['text'] = diff_model.tokenizer.get_tokens(text)\n",
    "image = diff_model.inference_generate_sample_with_condition(data=data, \n",
    "                                                            truncation_rate=TRUNCATION_RATE, \n",
    "                                                            batch_size=TIMES,  \n",
    "                                                            guidance_scale=GUIDANCE_SCALE, \n",
    "                                                            prior_rule=PRIOR_RULE, \n",
    "                                                            prior_weight=PRIOR_WEIGHT, \n",
    "                                                            learnable_cf=LEARNABLE_CF)\n",
    "img = to_pil_image(image)\n",
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HSD_vqd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
